


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs, make_gaussian_quantiles, make_moons
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import SGDClassifier
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import seaborn as sns
import warnings




















def split_and_plot(n_samples, centers, subplot_index):
    # 1. Generar dataset sintético con "centers" clases
    X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=1.2, random_state=42)

    # 2. Dividir en Train (60%) y resto (40%)
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.4, stratify=y, random_state=42
    )

    # 3. Dividir el resto en Dev (20%) y Test (20%)
    X_dev, X_test, y_dev, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42
    )

    # 4. Visualización
    plt.subplot(1, 3, subplot_index)
    plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap="viridis", marker="o", alpha=0.6, label="Train")#alpha define transparencia en los puntos 
    plt.scatter(X_dev[:,0], X_dev[:,1], c=y_dev, cmap="viridis", marker="s", alpha=0.6, label="Dev")
    plt.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap="viridis", marker="^", alpha=0.6, label="Test")
    plt.title(f"{centers} clases")
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    if subplot_index == 1:  # Solo mostrar leyenda en el primer gráfico
        plt.legend()

plt.figure(figsize=(15, 4))

# Dataset A: 2 clases
split_and_plot(n_samples=3000, centers=2, subplot_index=1)

# Dataset B: 3 clases
split_and_plot(n_samples=3000, centers=5, subplot_index=2)

# Dataset C: 5 clases
split_and_plot(n_samples=3000, centers=20, subplot_index=3)

plt.suptitle("Comparación de 3 datasets sintéticos con distinto número de clases", fontsize=14)
plt.tight_layout()
plt.show()





def generate_multiple_datasets(n_samples=3000, centers_list=[2,3,5], cluster_std=1.2, random_state=42):
    datasets = {}
    for centers in centers_list:
        X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std, random_state=random_state)
        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=random_state)
        X_dev, X_test,  y_dev,  y_test  = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=random_state)
        datasets[centers] = {"X_train": X_train, "y_train": y_train,
                             "X_dev":   X_dev,   "y_dev":   y_dev,
                             "X_test":  X_test,  "y_test":  y_test}
    return datasets


# --- modelos a comparar (con escalado donde aplica) ---
def get_models():
    return {        
        "KNN":    Pipeline([("scaler", StandardScaler()), ("clf", KNeighborsClassifier(n_neighbors=5))]),
        "SVM-RBF":Pipeline([("scaler", StandardScaler()), ("clf", SVC(kernel="rbf"))]),
        "DecisionTree": DecisionTreeClassifier(max_depth=5, random_state=42),                    
        "MLP":   Pipeline([("scaler", StandardScaler()), ("clf", MLPClassifier(hidden_layer_sizes=(50,), max_iter=500, random_state=42))]),
    }


datasets = generate_multiple_datasets(n_samples=12000, centers_list=[2,5,20], cluster_std=1.2, random_state=42)
models = get_models()

rows = []
for k_classes, data in datasets.items():
    X_train, y_train = data["X_train"], data["y_train"]
    X_test,  y_test  = data["X_test"],  data["y_test"]
    for name, clf in models.items():
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        f1m = f1_score(y_test, y_pred, average="macro")
        f1w = f1_score(y_test, y_pred, average="weighted")
        rows.append({"dataset_classes": k_classes, "model": name, "accuracy": acc, "f1_macro": f1m, "f1_weighted": f1w})

results = pd.DataFrame(rows).sort_values(["dataset_classes","f1_macro"], ascending=[True, False])
print(results)

# Vista rápida por dataset (mejor f1_macro)
print("\nTop por dataset (f1_macro):")
print(results.loc[results.groupby("dataset_classes")["f1_macro"].idxmax(), ["dataset_classes","model","f1_macro","accuracy"]])





pipe_knn = Pipeline([
    ("scaler", StandardScaler()),
    ("knn", KNeighborsClassifier())
])

param_grid = {
    "knn__n_neighbors": [1, 3, 5, 7, 9, 15, 21],
    "knn__weights": ["uniform", "distance"],
    "knn__metric": ["minkowski"],   # dejamos minkowski y movemos p
    "knn__p": [1, 2]                # 1=manhattan, 2=euclídea
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

gcv = GridSearchCV(
    pipe_knn,
    param_grid=param_grid,
    scoring="f1_macro",   # o 'accuracy', 'f1_weighted' según tu objetivo
    cv=cv,
    n_jobs=-1
)
gcv.fit(X_train, y_train)

print("Mejores params:", gcv.best_params_)
print("Mejor score CV:", round(gcv.best_score_, 3))

# Evalúa en DEV o TEST (según tu protocolo)
from sklearn.metrics import classification_report
y_pred = gcv.predict(X_dev)  # o X_test
print(classification_report(y_dev, y_pred))





param_grid = {
    "criterion": ["gini", "entropy"],
    "max_depth": [None, 3, 5, 7],
    "min_samples_split": [2, 10, 20],
    "min_samples_leaf": [1, 5, 20], #mínimo de muestras en cada hoja. Evita hojas diminutas (ruido).
    "max_features": [None, "sqrt", "log2"], #nº de variables candidatas por división. Limitarlo introduce regularización.
    "class_weight": [None, "balanced"],# "balanced" si hay desbalance de clases.
    "ccp_alpha": [0.0, 0.001, 0.01], # pruning cost–complexity (poda post-entrenamiento). >0 recorta ramas débiles.
}
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

rows = []
best_models = {}

for k_classes, d in datasets.items():
    Xtr, ytr = d["X_train"], d["y_train"]
    Xdv, ydv = d["X_dev"],   d["y_dev"]
    Xte, yte = d["X_test"],  d["y_test"]

    # Base (por defecto)
    base = DecisionTreeClassifier(random_state=42)
    base.fit(Xtr, ytr)
    yhat_dv_base = base.predict(Xdv)
    base_acc = accuracy_score(ydv, yhat_dv_base)
    base_f1m = f1_score(ydv, yhat_dv_base, average="macro")
    base_f1w = f1_score(ydv, yhat_dv_base, average="weighted")

    # Optimizado (GridSearch en DEV)
    gcv = GridSearchCV(DecisionTreeClassifier(random_state=42),
                       param_grid=param_grid, scoring="f1_macro",
                       cv=cv, n_jobs=-1, refit=True)
    gcv.fit(Xtr, ytr)
    best = gcv.best_estimator_
    yhat_dv_best = best.predict(Xdv)
    best_acc = accuracy_score(ydv, yhat_dv_best)
    best_f1m = f1_score(ydv, yhat_dv_best, average="macro")
    best_f1w = f1_score(ydv, yhat_dv_best, average="weighted")

    rows.append({
        "dataset_classes": k_classes,
        "model": "DT-Default", "split": "DEV",
        "accuracy": base_acc, "f1_macro": base_f1m, "f1_weighted": base_f1w
    })
    rows.append({
        "dataset_classes": k_classes,
        "model": "DT-Optimized", "split": "DEV",
        "accuracy": best_acc, "f1_macro": best_f1m, "f1_weighted": best_f1w
    })

    # Guarda el mejor para test
    best_models[k_classes] = best

# Evaluación final en TEST con el optimizado
for k_classes, d in datasets.items():
    Xte, yte = d["X_test"], d["y_test"]
    best = best_models[k_classes]
    yhat_te = best.predict(Xte)
    rows.append({
        "dataset_classes": k_classes,
        "model": "DT-Optimized", "split": "TEST",
        "accuracy": accuracy_score(yte, yhat_te),
        "f1_macro": f1_score(yte, yhat_te, average="macro"),
        "f1_weighted": f1_score(yte, yhat_te, average="weighted")
    })
    # (opcional) reporte detallado
    # print(f"\n=== TEST {k_classes} clases ===")
    # print(classification_report(yte, yhat_te))

results = pd.DataFrame(rows).sort_values(["dataset_classes","split","f1_macro"], ascending=[True, True, False])
print(results)

# Vista rápida: ganador en DEV por dataset
print("\nTop DEV por dataset:")
print(results[results["split"]=="DEV"].loc[results[results["split"]=="DEV"].groupby("dataset_classes")["f1_macro"].idxmax(),
      ["dataset_classes","model","accuracy","f1_macro"]])








# --- SVM con scaling y GridSearch ---
pipe_svm = Pipeline([
    ("scaler", StandardScaler()),
    ("svm", SVC(probability=False))  # para velocidad; si quieres ROC, usa True
])

param_grid = [
    {"svm__kernel": ["linear"], "svm__C": [0.1, 1, 10, 100]},#frontera recta, útil si las clases son linealmente separables.
    {"svm__kernel": ["rbf"],    "svm__C": [0.1, 1, 10, 100], "svm__gamma": ["scale", 0.1, 0.01, 0.001]},# frontera flexible, buena para problemas no lineales.
    # opcional: polinomial (suele ser más lento)
    # {"svm__kernel": ["poly"], "svm__degree":[2,3], "svm__C":[0.1,1,10], "svm__gamma":["scale", 0.1]}
]

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

rows = []
best_models = {}

for k_classes, d in datasets.items():
    Xtr, ytr = d["X_train"], d["y_train"]
    Xdv, ydv = d["X_dev"],   d["y_dev"]
    Xte, yte = d["X_test"],  d["y_test"]

    gcv = GridSearchCV(
        estimator=pipe_svm,
        param_grid=param_grid,
        scoring="f1_macro",   # multiclase: trata las clases por igual
        cv=cv,
        n_jobs=-1,
        refit=True,
        verbose=0
    )
    gcv.fit(Xtr, ytr)
    best = gcv.best_estimator_
    best_models[k_classes] = best

    # Evaluación en DEV
    yhat_dv = best.predict(Xdv)
    rows.append({
        "dataset_classes": k_classes, "model": "SVM-Optimized", "split": "DEV",
        "accuracy": accuracy_score(ydv, yhat_dv),
        "f1_macro": f1_score(ydv, yhat_dv, average="macro"),
        "f1_weighted": f1_score(ydv, yhat_dv, average="weighted"),
        "best_params": gcv.best_params_
    })

# Evaluación final en TEST
for k_classes, d in datasets.items():
    Xte, yte = d["X_test"], d["y_test"]
    best = best_models[k_classes]
    yhat_te = best.predict(Xte)
    rows.append({
        "dataset_classes": k_classes, "model": "SVM-Optimized", "split": "TEST",
        "accuracy": accuracy_score(yte, yhat_te),
        "f1_macro": f1_score(yte, yhat_te, average="macro"),
        "f1_weighted": f1_score(yte, yhat_te, average="weighted"),
        "best_params": best.get_params()
    })

results = pd.DataFrame(rows).sort_values(["dataset_classes","split","f1_macro"], ascending=[True, True, False])
print(results)

# Vista rápida: mejor SVM (DEV) por dataset y su rendimiento en TEST
print("\nTop DEV por dataset (SVM):")
print(results[results["split"]=="DEV"].loc[
    results[results["split"]=="DEV"].groupby("dataset_classes")["f1_macro"].idxmax(),
    ["dataset_classes","model","f1_macro","accuracy","best_params"]
])
print("\nResultados TEST (SVM optimizado):")
print(results[results["split"]=="TEST"].sort_values(["dataset_classes","f1_macro"], ascending=[True, False]))








# MLP necesita escalado
pipe_mlp = Pipeline([
    ("scaler", StandardScaler()),
    ("mlp", MLPClassifier(max_iter=500, random_state=42))
])

# Hiperparámetros a explorar
param_grid = {
    "mlp__hidden_layer_sizes": [(50,), (100,), (100,50)],# estructura de la red (nº de neuronas por capa).
    "mlp__activation": ["relu", "tanh"],# función de activación.
    "mlp__alpha": [0.0001, 0.001, 0.01],   # regularización L2 (para evitar sobreajuste).
    "mlp__learning_rate_init": [0.001, 0.01],  # tasa de aprendizaje (para Adam o SGD).
    "mlp__solver": ["adam"]   # dejamos Adam por practicidad optimizador.
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

rows = []
best_models = {}

for k_classes, d in datasets.items():
    Xtr, ytr = d["X_train"], d["y_train"]
    Xdv, ydv = d["X_dev"],   d["y_dev"]
    Xte, yte = d["X_test"],  d["y_test"]

    gcv = GridSearchCV(
        pipe_mlp, param_grid=param_grid,
        scoring="f1_macro", cv=cv, n_jobs=-1, refit=True, verbose=0
    )
    gcv.fit(Xtr, ytr)
    best = gcv.best_estimator_
    best_models[k_classes] = best

    # Evaluación en DEV
    yhat_dv = best.predict(Xdv)
    rows.append({
        "dataset_classes": k_classes, "model": "MLP-Optimized", "split": "DEV",
        "accuracy": accuracy_score(ydv, yhat_dv),
        "f1_macro": f1_score(ydv, yhat_dv, average="macro"),
        "f1_weighted": f1_score(ydv, yhat_dv, average="weighted"),
        "best_params": gcv.best_params_
    })

# Evaluación en TEST
for k_classes, d in datasets.items():
    Xte, yte = d["X_test"], d["y_test"]
    best = best_models[k_classes]
    yhat_te = best.predict(Xte)
    rows.append({
        "dataset_classes": k_classes, "model": "MLP-Optimized", "split": "TEST",
        "accuracy": accuracy_score(yte, yhat_te),
        "f1_macro": f1_score(yte, yhat_te, average="macro"),
        "f1_weighted": f1_score(yte, yhat_te, average="weighted"),
        "best_params": best.get_params()["mlp"]
    })

results = pd.DataFrame(rows).sort_values(["dataset_classes","split","f1_macro"], ascending=[True, True, False])
print(results)

# Vista rápida
print("\nTop DEV por dataset (MLP):")
print(results[results["split"]=="DEV"].loc[
    results[results["split"]=="DEV"].groupby("dataset_classes")["f1_macro"].idxmax(),
    ["dataset_classes","model","f1_macro","accuracy","best_params"]
])
print("\nResultados TEST (MLP optimizado):")
print(results[results["split"]=="TEST"].sort_values(["dataset_classes","f1_macro"], ascending=[True, False]))





# 3. Evaluación
rows = []
conf_matrices = {}

for k_classes, d in datasets.items():
    Xtr, ytr = d["X_train"], d["y_train"]
    Xte, yte = d["X_test"],  d["y_test"]

    for nombre, modelo in modelos.items():
        modelo.fit(Xtr, ytr)
        ypred = modelo.predict(Xte)

        acc = accuracy_score(yte, ypred)
        prec = precision_score(yte, ypred, average="macro")
        rec = recall_score(yte, ypred, average="macro")
        f1m = f1_score(yte, ypred, average="macro")
        f1w = f1_score(yte, ypred, average="weighted")

        rows.append({
            "dataset_classes": k_classes,
            "model": nombre,
            "accuracy": acc,
            "precision_macro": prec,
            "recall_macro": rec,
            "f1_macro": f1m,
            "f1_weighted": f1w
        })

        # Guardamos la matriz de confusión
        conf_matrices[(k_classes, nombre)] = confusion_matrix(yte, ypred)

# 4. Resultados en tabla
results = pd.DataFrame(rows).sort_values(["dataset_classes","f1_macro"], ascending=[True, False])
print(results)

# 5. Ejemplo: mostrar matrices de confusión de cada modelo en 3 clases
print("\nMATRICES DE CONFUSIÓN (dataset 3 clases):")
for nombre in modelos.keys():
    print(f"\nModelo: {nombre}")
    print(conf_matrices[(20, nombre)])


def plot_confusion_heatmap(cm, class_names, title="Matriz de confusión", normalize=False):
    if normalize:
        cm = cm.astype(float) / cm.sum(axis=1, keepdims=True).clip(min=1)
    fig, ax = plt.subplots(figsize=(4,3))
    im = ax.imshow(cm, aspect="auto")
    ax.set_title(title)
    ax.set_xlabel("Predicción")
    ax.set_ylabel("Real")
    ax.set_xticks(range(len(class_names))); ax.set_xticklabels(class_names, rotation=45, ha="right")
    ax.set_yticks(range(len(class_names))); ax.set_yticklabels(class_names)
    # anotaciones
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, f"{cm[i, j]:.2f}" if normalize else f"{cm[i, j]}",
                    ha="center", va="center", fontsize=9, color="white" if im.norm(cm[i,j])>0.5 else "black")
    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    plt.tight_layout()
    plt.show()

# Ejemplo: mostrar heatmaps para TODOS los modelos del dataset de 3 clases
k = 5
class_names = sorted(np.unique(datasets[k]["y_test"]))
for model_name in modelos.keys():
    cm = conf_matrices[(k, model_name)]
    plot_confusion_heatmap(cm, class_names, title=f"{model_name} | {k} clases (sin normalizar)", normalize=False)
    plot_confusion_heatmap(cm, class_names, title=f"{model_name} | {k} clases (normalizada por fila)", normalize=True)


def barplot_metrics(results, k_classes, metrics=("accuracy","precision_macro","recall_macro","f1_macro")):
    df = results[results["dataset_classes"]==k_classes].copy()
    # si tienes filas DEV y TEST, filtra una (ej. TEST) para comparar final
    if "split" in df.columns:
        df = df[df["split"]=="TEST"].copy()
    models = df["model"].tolist()
    x = np.arange(len(models))
    width = 0.18

    fig, ax = plt.subplots(figsize=(12,4))
    for i, m in enumerate(metrics):
        ax.bar(x + i*width, df[m].values, width, label=m)
    ax.set_xticks(x + width*(len(metrics)-1)/2)
    ax.set_xticklabels(models, rotation=30, ha="right")
    ax.set_ylim(0, 1.0)
    ax.set_title(f"Métricas en TEST | {k_classes} clases")
    ax.legend()
    ax.grid(axis="y", alpha=0.3)
    plt.tight_layout()
    plt.show()

# Barras para 2, 3 y 5 clases (puedes cambiar el set de métricas si quieres)
for k in [2,5,20]:
    barplot_metrics(results, k, metrics=("accuracy","precision_macro","recall_macro","f1_macro"))


def plot_decision_boundary(ax, clf, X, y, title=None, proba=False, padding=0.05, grid=300):
    # Rango con pequeño padding
    x_min, x_max = X[:,0].min(), X[:,0].max()
    y_min, y_max = X[:,1].min(), X[:,1].max()
    dx, dy = x_max - x_min, y_max - y_min
    x_min, x_max = x_min - dx*padding, x_max + dx*padding
    y_min, y_max = y_min - dy*padding, y_max + dy*padding

    xx, yy = np.meshgrid(np.linspace(x_min, x_max, grid),
                         np.linspace(y_min, y_max, grid))
    grid_xy = np.c_[xx.ravel(), yy.ravel()]

    # Predicción en rejilla
    if proba and hasattr(clf, "predict_proba"):
        Z = clf.predict_proba(grid_xy).argmax(1)  # clase más probable
    else:
        Z = clf.predict(grid_xy)
    Z = Z.reshape(xx.shape)

    # Mapa de decisión + puntos
    cs = ax.contourf(xx, yy, Z, alpha=0.25, levels=np.unique(Z).size)
    sc = ax.scatter(X[:,0], X[:,1], c=y, s=20, alpha=0.8)
    ax.set_title(title or type(clf).__name__)
    ax.set_xlabel("Feature 1"); ax.set_ylabel("Feature 2")

def visualize_many_classifiers(classifiers, X, y, cols=3, fit=True):
    """
    classifiers: lista de (nombre, estimator) — usa Pipeline con StandardScaler si el modelo lo necesita.
    X, y: datos 2D (si tienes >2 features, proyecta antes con PCA/UMAP).
    """
    n = len(classifiers)
    rows = int(np.ceil(n/cols))
    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 3.5*rows))
    axes = np.array(axes).reshape(-1)

    for i, (name, clf) in enumerate(classifiers):
        if fit:
            clf.fit(X, y)
        plot_decision_boundary(axes[i], clf, X, y, title=name)
    # ocultar subplots vacíos
    for j in range(i+1, rows*cols):
        axes[j].axis("off")
    plt.tight_layout(); plt.show()


# Datos 2D
X_a, y_a = make_blobs(n_samples=1249, centers=2, cluster_std=1.2, random_state=42)
X_b, y_b = make_blobs(n_samples=1249, centers=5, cluster_std=1.2, random_state=42)
X_c, y_c = make_blobs(n_samples=1249, centers=20, cluster_std=1.2, random_state=42)

clfs = [
    
    ("KNN-7",  Pipeline([("sc", StandardScaler()),
                         ("clf", KNeighborsClassifier(n_neighbors=7))])),
    
    ("SVM-RBF",Pipeline([("sc", StandardScaler()),
                         ("clf", SVC(kernel="rbf", C=10, gamma="scale"))])),
    
    ("Árbol",  DecisionTreeClassifier(max_depth=5, random_state=42)),
    
    ("MLP",    Pipeline([("sc", StandardScaler()),
                         ("clf", MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42))])),
]

visualize_many_classifiers(clfs, X_a, y_a, cols=3, fit=True)
visualize_many_classifiers(clfs, X_b, y_b, cols=3, fit=True)
visualize_many_classifiers(clfs, X_c, y_c, cols=3, fit=True)














warnings.filterwarnings("ignore")
RND = 42

RUN_SIZES = [10**2, 10**3, 10**4, 10**5]
USE_1M = False
if USE_1M and 10**6 not in RUN_SIZES:
    RUN_SIZES.append(10**6)

# Maxímo de filas para usar modelos pesados (SVM-RBF, KNN, MLP)
MAX_ROWS_FOR_HEAVY_MODELS = 200_000 # sustitución por modelos lineale






def gen_and_split(n_samples, n_features=2, n_classes=4, random_state=RND):
    X, y = make_gaussian_quantiles(n_samples=n_samples, n_features=n_features,
                                   n_classes=n_classes, random_state=random_state)
    # split 60/20/20 train/dev/test
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=random_state)
    X_dev, X_test, y_dev, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=random_state)
    return X_train, X_dev, X_test, y_train, y_dev, y_test






base_models = {
    "kNN": (KNeighborsClassifier(), {"n_neighbors":[3,5,7]}),
    "DecisionTree": (DecisionTreeClassifier(random_state=RND), {"max_depth":[3,5,10,None], "min_samples_leaf":[1,2,5]}),
    "SVM": (SVC(), {"C":[0.1,1], "kernel":["linear","rbf"]}),
    "MLP": (MLPClassifier(max_iter=300, random_state=RND), {"hidden_layer_sizes":[(50,),(100,)], "learning_rate_init":[0.001]})
}

large_models = {
    "kNN": (KNeighborsClassifier(algorithm='ball_tree', n_jobs=-1), {"n_neighbors":[3,5]}),
    "DecisionTree": (DecisionTreeClassifier(random_state=RND), {"max_depth":[10, None], "min_samples_leaf":[1,5]}),
    "SGD_log": (SGDClassifier(loss='log', max_iter=1000, tol=1e-3, random_state=RND), {"alpha":[1e-4,1e-3]}),  # proxy para SVM/LogReg
}





def run_experiment(sizes, use_large_subs=True, random_state=RND):
    results = []
    details = []  # para guardar reports si quieres
    for n in sizes:
        print(f"\n--- Size: {n} samples ---")
        X_train, X_dev, X_test, y_train, y_dev, y_test = gen_and_split(n)
        n_train = X_train.shape[0]
        print(f"Train/dev/test sizes: {X_train.shape[0]}/{X_dev.shape[0]}/{X_test.shape[0]}")
        
        # Decide los modelos
        if use_large_subs and n > MAX_ROWS_FOR_HEAVY_MODELS:
            models = large_models
            print("Usando configuración para datasets grandes (modelos ligeros).")
        else:
            models = base_models
        
        for name, (model, grid) in models.items():
            print(f" -> Model: {name}")
            # GridSearch sobre X_train
            grid_search = GridSearchCV(model, grid, cv=3, scoring='accuracy', n_jobs=-1, refit=True)
            try:
                grid_search.fit(X_train, y_train)
                best = grid_search.best_estimator_
            except Exception as e:
                print(f"   ERROR al entrenar {name}: {e}. Intentando fit directo con parámetros por defecto.")
                try:
                    model.fit(X_train, y_train)
                    best = model
                except Exception as e2:
                    print(f"   Falló también fit directo: {e2}. Se salta este modelo.")
                    continue

            # Evaluación
            y_dev_pred = best.predict(X_dev)
            y_test_pred = best.predict(X_test)
            acc_dev = accuracy_score(y_dev, y_dev_pred)
            acc_test = accuracy_score(y_test, y_test_pred)
            f1_dev = f1_score(y_dev, y_dev_pred, average='macro')
            f1_test = f1_score(y_test, y_test_pred, average='macro')

            results.append({
                "n_samples": n,
                "model": name,
                "best_params": getattr(grid_search, "best_params_", None),
                "acc_dev": acc_dev,
                "acc_test": acc_test,
                "f1_dev": f1_dev,
                "f1_test": f1_test,
                "n_train": n_train
            })
            # Guardar reportes detallados
            details.append({
                "n_samples": n,
                "model": name,
                "classification_report_test": classification_report(y_test, y_test_pred, zero_division=0),
                "confusion_matrix_test": confusion_matrix(y_test, y_test_pred)
            })
            print(f"    acc_dev={acc_dev:.4f}  acc_test={acc_test:.4f}  f1_test={f1_test:.4f}")

    df = pd.DataFrame(results)
    return df, details






df_results, details = run_experiment(RUN_SIZES, use_large_subs=True)
print("\n== Resultados resumidos ==")
display(df_results)

# Guardado de resultados
df_results, details = run_experiment(RUN_SIZES, use_large_subs=True)





# =========================
# Plot: Barplot de Accuracy (test) por tamaño y modelo
# =========================
plt.figure(figsize=(10,6))
sns.barplot(data=df_results, x="n_samples", y="acc_test", hue="model")
plt.title("Accuracy (test) por modelo y tamaño de dataset")
plt.ylabel("Accuracy (test)")
plt.xlabel("n_samples")
plt.ylim(0,1)
plt.legend(bbox_to_anchor=(1.02,1), loc="upper left")
plt.show()

# =========================
# Heatmap: pivot tabla
# =========================
pivot = df_results.pivot_table(index="model", columns="n_samples", values="acc_test")
plt.figure(figsize=(8,4))
sns.heatmap(pivot, annot=True, fmt=".3f", cmap="viridis")
plt.title("Heatmap: Accuracy (test)")
plt.show()

# =========================
# Radar plot por modelo - comparando tamaños
# =========================
def radar_plot_for_model(df, model_name):
    from math import pi
    d = df[df['model']==model_name].sort_values('n_samples')
    values = d['acc_test'].values
    labels = d['n_samples'].astype(str).tolist()
    N = len(values)
    angles = [n / float(N) * 2 * pi for n in range(N)]
    values = np.concatenate((values, [values[0]]))
    angles = angles + [angles[0]]
    plt.figure(figsize=(6,6))
    ax = plt.subplot(111, polar=True)
    ax.plot(angles, values, linewidth=2)
    ax.fill(angles, values, alpha=0.25)
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(labels)
    ax.set_title(f"Radar: {model_name} (accuracy test)")
    ax.set_ylim(0,1)
    plt.show()


radar_plot_for_model(df_results, "DecisionTree")

radar_plot_for_model(df_results, "kNN")

radar_plot_for_model(df_results, "MLP")

radar_plot_for_model(df_results, "SVM")











# Generamos 1000 muestras con algo de ruido gaussiano para que no sea trivialmente separable.
X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)

# --- 2. División Estratificada Inicial ---
# Dividimos en entrenamiento (60%), desarrollo (20%) y prueba (20%).
# La estratificación asegura que la proporción de clases se mantenga en todas las divisiones.
X_train_base, X_temp, y_train_base, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)
X_dev, X_test, y_dev_base, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# --- 3. Función para Introducir Ruido ---
def introducir_ruido(y_labels, porcentaje_ruido):
    """
    Invierte un porcentaje de las etiquetas en un array.
    """
    if porcentaje_ruido == 0:
        return y_labels.copy()
    
    y_ruidoso = y_labels.copy()
    n_muestras = len(y_ruidoso)
    n_ruido = int(n_muestras * porcentaje_ruido)
    
    # Seleccionamos índices al azar para corromper
    np.random.seed(42) # Para reproducibilidad
    indices_ruidosos = np.random.choice(np.arange(n_muestras), size=n_ruido, replace=False)
    
    # Invertimos las etiquetas en esos índices (0 -> 1, 1 -> 0)
    y_ruidoso[indices_ruidosos] = 1 - y_ruidoso[indices_ruidosos]
    
    return y_ruidoso

# --- 4. Creación de los Datasets con Ruido ---
niveles_ruido = [0.0, 0.05, 0.10, 0.20, 0.30]
datasets_ruidosos = {}

for ruido in niveles_ruido:
    # Aplicamos ruido SOLO a los conjuntos de entrenamiento y desarrollo
    y_train_ruidoso = introducir_ruido(y_train_base, ruido)
    y_dev_ruidoso = introducir_ruido(y_dev_base, ruido)
    
    datasets_ruidosos[ruido] = {
        'X_train': X_train_base, 'y_train': y_train_ruidoso,
        'X_dev': X_dev, 'y_dev': y_dev_ruidoso,
        'X_test': X_test, 'y_test': y_test # El conjunto de test siempre es limpio
    }

# --- 5. Visualización del Dataset Limpio ---
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_train_base[:, 0], y=X_train_base[:, 1], hue=y_train_base, palette='viridis', s=50, alpha=0.7)
plt.title('Conjunto de Datos "Moons" (Limpio)', fontsize=16)
plt.xlabel('Característica 1', fontsize=12)
plt.ylabel('Característica 2', fontsize=12)
plt.legend(title='Clase')
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

# Verifiquemos el tamaño de los conjuntos
print(f"Tamaño del conjunto de entrenamiento: {len(X_train_base)}")
print(f"Tamaño del conjunto de desarrollo: {len(X_dev)}")
print(f"Tamaño del conjunto de prueba: {len(X_test)}")





# --- 1. Definición de Clasificadores y Rejillas de Parámetros ---

# Clasificadores a evaluar
classifiers = {
    'k-NN': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'SVM': SVC(random_state=42, probability=True), # probability=True para algunas visualizaciones si se necesitaran
    'MLP': MLPClassifier(random_state=42, max_iter=1000) # Aumentamos max_iter para asegurar convergencia
}

# Rejillas de hiperparámetros para la búsqueda
param_grids = {
    'k-NN': {'n_neighbors': [3, 5, 7, 9, 11, 15]},
    'Decision Tree': {'max_depth': [None, 3, 5, 7, 10]},
    'SVM': {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001]},
    'MLP': {
        'hidden_layer_sizes': [(50,), (100,), (50, 50)],
        'alpha': [0.0001, 0.001, 0.01],
        'learning_rate_init': [0.001, 0.01]
    }
}

# --- 2. Proceso de Búsqueda y Entrenamiento ---

best_models = {}
import time

for ruido in niveles_ruido:
    print(f"--- Procesando Nivel de Ruido: {ruido*100:.0f}% ---")
    best_models[ruido] = {}
    
    # Unimos los datos de entrenamiento y desarrollo para la validación cruzada
    X_train_dev = np.vstack((datasets_ruidosos[ruido]['X_train'], datasets_ruidosos[ruido]['X_dev']))
    y_train_dev = np.hstack((datasets_ruidosos[ruido]['y_train'], datasets_ruidosos[ruido]['y_dev']))
    
    for name in classifiers.keys():
        start_time = time.time()
        print(f"  Ajustando {name}...")
        
        X_scaled = X_train_dev.copy()
        # Escalamos los datos para SVM y MLP
        if name in ['SVM', 'MLP']:
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X_train_dev)
        
        # Configuración de GridSearchCV
        grid_search = GridSearchCV(
            classifiers[name], 
            param_grids[name], 
            cv=5, # 5-fold cross-validation
            scoring='accuracy', 
            n_jobs=-1 # Usar todos los cores disponibles
        )
        
        # Ejecutamos la búsqueda
        grid_search.fit(X_scaled, y_train_dev)
        
        # Guardamos el mejor modelo encontrado
        best_models[ruido][name] = grid_search.best_estimator_
        
        end_time = time.time()
        print(f"    Mejores parámetros: {grid_search.best_params_}")
        print(f"    Mejor accuracy (CV): {grid_search.best_score_:.4f}")
        print(f"    Tiempo: {end_time - start_time:.2f} segundos")

print("\n--- Proceso de ajuste de hiperparámetros completado. ---")






results = []

X_test = datasets_ruidosos[0.0]['X_test']
y_test = datasets_ruidosos[0.0]['y_test']

for ruido in niveles_ruido:
    for name, model in best_models[ruido].items():
        
        X_test_scaled = X_test.copy()
        
   
        if name in ['SVM', 'MLP']:
            scaler = StandardScaler()
            X_train_base = datasets_ruidosos[ruido]['X_train'] # Usamos el train set correspondiente
            scaler.fit(X_train_base)
            X_test_scaled = scaler.transform(X_test)
            
        # Realizamos predicciones
        y_pred = model.predict(X_test_scaled)
        
        # Calculamos métricas
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')
        cm = confusion_matrix(y_test, y_pred)
        
        results.append({
            'ruido': f"{ruido*100:.0f}%",
            'modelo': name,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'confusion_matrix': cm
        })


results_df = pd.DataFrame(results)


plt.style.use('seaborn-v0_8-whitegrid')
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))

# Gráfico de Accuracy
sns.lineplot(data=results_df, x='ruido', y='accuracy', hue='modelo', marker='o', ax=ax1)
ax1.set_title('Accuracy vs. Nivel de Ruido en Etiquetas', fontsize=16)
ax1.set_xlabel('Porcentaje de Ruido', fontsize=12)
ax1.set_ylabel('Accuracy en Test Set', fontsize=12)
ax1.legend(title='Modelo')
ax1.set_ylim(0.5, 1.0) # Ajustar límite para mejor visualización

# Gráfico de F1-Score
sns.lineplot(data=results_df, x='ruido', y='f1_score', hue='modelo', marker='o', ax=ax2)
ax2.set_title('F1-Score vs. Nivel de Ruido en Etiquetas', fontsize=16)
ax2.set_xlabel('Porcentaje de Ruido', fontsize=12)
ax2.set_ylabel('F1-Score en Test Set', fontsize=12)
ax2.legend(title='Modelo')
ax2.set_ylim(0.5, 1.0)

plt.tight_layout()
plt.show()


print("Resultados detallados en el conjunto de prueba:")
display(results_df[['ruido', 'modelo', 'accuracy', 'f1_score']].sort_values(by=['ruido', 'accuracy'], ascending=[True, False]))


ruidos_a_visualizar = ['0%', '5%','10%','20%', '30%']
modelos_a_visualizar = list(classifiers.keys())

n_modelos = len(modelos_a_visualizar)
fig, axes = plt.subplots(len(ruidos_a_visualizar), n_modelos, figsize=(20, 10))

for i, ruido_str in enumerate(ruidos_a_visualizar):
    for j, modelo_str in enumerate(modelos_a_visualizar):
        ax = axes[i, j]
        
        res = results_df[(results_df['ruido'] == ruido_str) & (results_df['modelo'] == modelo_str)]
        
        if not res.empty:
            cm = res.iloc[0]['confusion_matrix']
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False)
            ax.set_title(f'{modelo_str}\nRuido: {ruido_str}', fontsize=14)
            ax.set_xlabel('Predicho')
            ax.set_ylabel('Verdadero')
        else:
            ax.axis('off') # Ocultar si no hay datos

fig.suptitle('Matrices de Confusión: Sin Ruido vs. Máximo Ruido', fontsize=20, y=1.03)
plt.tight_layout()
plt.show()








warnings.filterwarnings('ignore', category=UserWarning, module='plotly')

df_radar = results_df[results_df['ruido'] == '20%'].copy()
metrics = ['accuracy', 'precision', 'recall', 'f1_score']

fig = go.Figure()

for i, row in df_radar.iterrows():
    fig.add_trace(go.Scatterpolar(
        r=row[metrics].values,
        theta=metrics,
        fill='toself',
        name=row['modelo']
    ))

fig.update_layout(
    polar=dict(
        radialaxis=dict(
            visible=True,
            range=[0.6, 1.0]
        )),
    showlegend=True,
    title='Comparación Multimétrica de Modelos con 20% de Ruido',
    template='plotly_dark'
)
fig.show()



df_bar = results_df[results_df['ruido'] == '30%'].melt(
    id_vars='modelo', 
    value_vars=metrics,
    var_name='metrica',
    value_name='valor'
)

plt.figure(figsize=(14, 8))
sns.barplot(data=df_bar, x='metrica', y='valor', hue='modelo', palette='viridis')
plt.title('Comparación de Métricas en el Peor Escenario (30% Ruido)', fontsize=16)
plt.xlabel('Métrica', fontsize=12)
plt.ylabel('Valor', fontsize=12)
plt.ylim(0.5, 1.0)
plt.legend(title='Modelo', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()



def plot_decision_boundary(modelo, X, y, ax, title):
    """Función para visualizar la frontera de decisión de un modelo."""
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                         np.arange(y_min, y_max, 0.02))

    Z = modelo.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    ax.contourf(xx, yy, Z, alpha=0.4, cmap='viridis')
    ax.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k', cmap='viridis')
    ax.set_title(title, fontsize=12)
    ax.set_xlabel('Característica 1')
    ax.set_ylabel('Característica 2')

ruidos_vis = [0.0, 0.30]
modelos_vis = list(classifiers.keys())
fig, axes = plt.subplots(len(ruidos_vis), len(modelos_vis), figsize=(22, 10))

for i, ruido in enumerate(ruidos_vis):
    X_train_vis = datasets_ruidosos[ruido]['X_train']
    y_train_vis = datasets_ruidosos[ruido]['y_train']
    
    for j, name in enumerate(modelos_vis):
        ax = axes[i, j]
        model = best_models[ruido][name]
        
        X_vis_scaled = X_train_vis.copy()
        
        if name in ['SVM', 'MLP']:
            scaler = StandardScaler().fit(X_train_vis)
            X_vis_scaled = scaler.transform(X_train_vis)
        
        plot_decision_boundary(model, X_vis_scaled, y_train_vis, ax, f'{name} (Ruido: {ruido*100:.0f}%)')

fig.suptitle('Fronteras de Decisión Aprendidas (Datos de Entrenamiento)', fontsize=20, y=1.02)
plt.tight_layout()
plt.show()





















