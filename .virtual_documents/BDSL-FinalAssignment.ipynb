import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs, make_gaussian_quantiles
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import SGDClassifier
import seaborn as sns
import warnings




















def split_and_plot(n_samples, centers, subplot_index):
    # 1. Generar dataset sintético con "centers" clases
    X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=1.2, random_state=42)

    # 2. Dividir en Train (60%) y resto (40%)
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.4, stratify=y, random_state=42
    )

    # 3. Dividir el resto en Dev (20%) y Test (20%)
    X_dev, X_test, y_dev, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42
    )

    # 4. Visualización
    plt.subplot(1, 3, subplot_index)
    plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap="viridis", marker="o", alpha=0.6, label="Train")#alpha define transparencia en los puntos 
    plt.scatter(X_dev[:,0], X_dev[:,1], c=y_dev, cmap="viridis", marker="s", alpha=0.6, label="Dev")
    plt.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap="viridis", marker="^", alpha=0.6, label="Test")
    plt.title(f"{centers} clases")
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    if subplot_index == 1:  # Solo mostrar leyenda en el primer gráfico
        plt.legend()

plt.figure(figsize=(15, 4))

# Dataset A: 2 clases
split_and_plot(n_samples=3000, centers=2, subplot_index=1)

# Dataset B: 3 clases
split_and_plot(n_samples=3000, centers=5, subplot_index=2)

# Dataset C: 5 clases
split_and_plot(n_samples=3000, centers=20, subplot_index=3)

plt.suptitle("Comparación de 3 datasets sintéticos con distinto número de clases", fontsize=14)
plt.tight_layout()
plt.show()





def generate_multiple_datasets(n_samples=3000, centers_list=[2,3,5], cluster_std=1.2, random_state=42):
    datasets = {}
    for centers in centers_list:
        X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std, random_state=random_state)
        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=random_state)
        X_dev, X_test,  y_dev,  y_test  = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=random_state)
        datasets[centers] = {"X_train": X_train, "y_train": y_train,
                             "X_dev":   X_dev,   "y_dev":   y_dev,
                             "X_test":  X_test,  "y_test":  y_test}
    return datasets


# --- modelos a comparar (con escalado donde aplica) ---
def get_models():
    return {        
        "KNN":    Pipeline([("scaler", StandardScaler()), ("clf", KNeighborsClassifier(n_neighbors=5))]),
        "SVM-RBF":Pipeline([("scaler", StandardScaler()), ("clf", SVC(kernel="rbf"))]),
        "DecisionTree": DecisionTreeClassifier(max_depth=5, random_state=42),                    
        "MLP":   Pipeline([("scaler", StandardScaler()), ("clf", MLPClassifier(hidden_layer_sizes=(50,), max_iter=500, random_state=42))]),
    }


datasets = generate_multiple_datasets(n_samples=12000, centers_list=[2,5,20], cluster_std=1.2, random_state=42)
models = get_models()

rows = []
for k_classes, data in datasets.items():
    X_train, y_train = data["X_train"], data["y_train"]
    X_test,  y_test  = data["X_test"],  data["y_test"]
    for name, clf in models.items():
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        f1m = f1_score(y_test, y_pred, average="macro")
        f1w = f1_score(y_test, y_pred, average="weighted")
        rows.append({"dataset_classes": k_classes, "model": name, "accuracy": acc, "f1_macro": f1m, "f1_weighted": f1w})

results = pd.DataFrame(rows).sort_values(["dataset_classes","f1_macro"], ascending=[True, False])
print(results)

# Vista rápida por dataset (mejor f1_macro)
print("\nTop por dataset (f1_macro):")
print(results.loc[results.groupby("dataset_classes")["f1_macro"].idxmax(), ["dataset_classes","model","f1_macro","accuracy"]])





pipe_knn = Pipeline([
    ("scaler", StandardScaler()),
    ("knn", KNeighborsClassifier())
])

param_grid = {
    "knn__n_neighbors": [1, 3, 5, 7, 9, 15, 21],
    "knn__weights": ["uniform", "distance"],
    "knn__metric": ["minkowski"],   # dejamos minkowski y movemos p
    "knn__p": [1, 2]                # 1=manhattan, 2=euclídea
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

gcv = GridSearchCV(
    pipe_knn,
    param_grid=param_grid,
    scoring="f1_macro",   # o 'accuracy', 'f1_weighted' según tu objetivo
    cv=cv,
    n_jobs=-1
)
gcv.fit(X_train, y_train)

print("Mejores params:", gcv.best_params_)
print("Mejor score CV:", round(gcv.best_score_, 3))

# Evalúa en DEV o TEST (según tu protocolo)
from sklearn.metrics import classification_report
y_pred = gcv.predict(X_dev)  # o X_test
print(classification_report(y_dev, y_pred))





param_grid = {
    "criterion": ["gini", "entropy"],
    "max_depth": [None, 3, 5, 7],
    "min_samples_split": [2, 10, 20],
    "min_samples_leaf": [1, 5, 20], #mínimo de muestras en cada hoja. Evita hojas diminutas (ruido).
    "max_features": [None, "sqrt", "log2"], #nº de variables candidatas por división. Limitarlo introduce regularización.
    "class_weight": [None, "balanced"],# "balanced" si hay desbalance de clases.
    "ccp_alpha": [0.0, 0.001, 0.01], # pruning cost–complexity (poda post-entrenamiento). >0 recorta ramas débiles.
}
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

rows = []
best_models = {}

for k_classes, d in datasets.items():
    Xtr, ytr = d["X_train"], d["y_train"]
    Xdv, ydv = d["X_dev"],   d["y_dev"]
    Xte, yte = d["X_test"],  d["y_test"]

    # Base (por defecto)
    base = DecisionTreeClassifier(random_state=42)
    base.fit(Xtr, ytr)
    yhat_dv_base = base.predict(Xdv)
    base_acc = accuracy_score(ydv, yhat_dv_base)
    base_f1m = f1_score(ydv, yhat_dv_base, average="macro")
    base_f1w = f1_score(ydv, yhat_dv_base, average="weighted")

    # Optimizado (GridSearch en DEV)
    gcv = GridSearchCV(DecisionTreeClassifier(random_state=42),
                       param_grid=param_grid, scoring="f1_macro",
                       cv=cv, n_jobs=-1, refit=True)
    gcv.fit(Xtr, ytr)
    best = gcv.best_estimator_
    yhat_dv_best = best.predict(Xdv)
    best_acc = accuracy_score(ydv, yhat_dv_best)
    best_f1m = f1_score(ydv, yhat_dv_best, average="macro")
    best_f1w = f1_score(ydv, yhat_dv_best, average="weighted")

    rows.append({
        "dataset_classes": k_classes,
        "model": "DT-Default", "split": "DEV",
        "accuracy": base_acc, "f1_macro": base_f1m, "f1_weighted": base_f1w
    })
    rows.append({
        "dataset_classes": k_classes,
        "model": "DT-Optimized", "split": "DEV",
        "accuracy": best_acc, "f1_macro": best_f1m, "f1_weighted": best_f1w
    })

    # Guarda el mejor para test
    best_models[k_classes] = best

# Evaluación final en TEST con el optimizado
for k_classes, d in datasets.items():
    Xte, yte = d["X_test"], d["y_test"]
    best = best_models[k_classes]
    yhat_te = best.predict(Xte)
    rows.append({
        "dataset_classes": k_classes,
        "model": "DT-Optimized", "split": "TEST",
        "accuracy": accuracy_score(yte, yhat_te),
        "f1_macro": f1_score(yte, yhat_te, average="macro"),
        "f1_weighted": f1_score(yte, yhat_te, average="weighted")
    })
    # (opcional) reporte detallado
    # print(f"\n=== TEST {k_classes} clases ===")
    # print(classification_report(yte, yhat_te))

results = pd.DataFrame(rows).sort_values(["dataset_classes","split","f1_macro"], ascending=[True, True, False])
print(results)

# Vista rápida: ganador en DEV por dataset
print("\nTop DEV por dataset:")
print(results[results["split"]=="DEV"].loc[results[results["split"]=="DEV"].groupby("dataset_classes")["f1_macro"].idxmax(),
      ["dataset_classes","model","accuracy","f1_macro"]])








# --- SVM con scaling y GridSearch ---
pipe_svm = Pipeline([
    ("scaler", StandardScaler()),
    ("svm", SVC(probability=False))  # para velocidad; si quieres ROC, usa True
])

param_grid = [
    {"svm__kernel": ["linear"], "svm__C": [0.1, 1, 10, 100]},#frontera recta, útil si las clases son linealmente separables.
    {"svm__kernel": ["rbf"],    "svm__C": [0.1, 1, 10, 100], "svm__gamma": ["scale", 0.1, 0.01, 0.001]},# frontera flexible, buena para problemas no lineales.
    # opcional: polinomial (suele ser más lento)
    # {"svm__kernel": ["poly"], "svm__degree":[2,3], "svm__C":[0.1,1,10], "svm__gamma":["scale", 0.1]}
]

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

rows = []
best_models = {}

for k_classes, d in datasets.items():
    Xtr, ytr = d["X_train"], d["y_train"]
    Xdv, ydv = d["X_dev"],   d["y_dev"]
    Xte, yte = d["X_test"],  d["y_test"]

    gcv = GridSearchCV(
        estimator=pipe_svm,
        param_grid=param_grid,
        scoring="f1_macro",   # multiclase: trata las clases por igual
        cv=cv,
        n_jobs=-1,
        refit=True,
        verbose=0
    )
    gcv.fit(Xtr, ytr)
    best = gcv.best_estimator_
    best_models[k_classes] = best

    # Evaluación en DEV
    yhat_dv = best.predict(Xdv)
    rows.append({
        "dataset_classes": k_classes, "model": "SVM-Optimized", "split": "DEV",
        "accuracy": accuracy_score(ydv, yhat_dv),
        "f1_macro": f1_score(ydv, yhat_dv, average="macro"),
        "f1_weighted": f1_score(ydv, yhat_dv, average="weighted"),
        "best_params": gcv.best_params_
    })

# Evaluación final en TEST
for k_classes, d in datasets.items():
    Xte, yte = d["X_test"], d["y_test"]
    best = best_models[k_classes]
    yhat_te = best.predict(Xte)
    rows.append({
        "dataset_classes": k_classes, "model": "SVM-Optimized", "split": "TEST",
        "accuracy": accuracy_score(yte, yhat_te),
        "f1_macro": f1_score(yte, yhat_te, average="macro"),
        "f1_weighted": f1_score(yte, yhat_te, average="weighted"),
        "best_params": best.get_params()
    })

results = pd.DataFrame(rows).sort_values(["dataset_classes","split","f1_macro"], ascending=[True, True, False])
print(results)

# Vista rápida: mejor SVM (DEV) por dataset y su rendimiento en TEST
print("\nTop DEV por dataset (SVM):")
print(results[results["split"]=="DEV"].loc[
    results[results["split"]=="DEV"].groupby("dataset_classes")["f1_macro"].idxmax(),
    ["dataset_classes","model","f1_macro","accuracy","best_params"]
])
print("\nResultados TEST (SVM optimizado):")
print(results[results["split"]=="TEST"].sort_values(["dataset_classes","f1_macro"], ascending=[True, False]))








# MLP necesita escalado
pipe_mlp = Pipeline([
    ("scaler", StandardScaler()),
    ("mlp", MLPClassifier(max_iter=500, random_state=42))
])

# Hiperparámetros a explorar
param_grid = {
    "mlp__hidden_layer_sizes": [(50,), (100,), (100,50)],# estructura de la red (nº de neuronas por capa).
    "mlp__activation": ["relu", "tanh"],# función de activación.
    "mlp__alpha": [0.0001, 0.001, 0.01],   # regularización L2 (para evitar sobreajuste).
    "mlp__learning_rate_init": [0.001, 0.01],  # tasa de aprendizaje (para Adam o SGD).
    "mlp__solver": ["adam"]   # dejamos Adam por practicidad optimizador.
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

rows = []
best_models = {}

for k_classes, d in datasets.items():
    Xtr, ytr = d["X_train"], d["y_train"]
    Xdv, ydv = d["X_dev"],   d["y_dev"]
    Xte, yte = d["X_test"],  d["y_test"]

    gcv = GridSearchCV(
        pipe_mlp, param_grid=param_grid,
        scoring="f1_macro", cv=cv, n_jobs=-1, refit=True, verbose=0
    )
    gcv.fit(Xtr, ytr)
    best = gcv.best_estimator_
    best_models[k_classes] = best

    # Evaluación en DEV
    yhat_dv = best.predict(Xdv)
    rows.append({
        "dataset_classes": k_classes, "model": "MLP-Optimized", "split": "DEV",
        "accuracy": accuracy_score(ydv, yhat_dv),
        "f1_macro": f1_score(ydv, yhat_dv, average="macro"),
        "f1_weighted": f1_score(ydv, yhat_dv, average="weighted"),
        "best_params": gcv.best_params_
    })

# Evaluación en TEST
for k_classes, d in datasets.items():
    Xte, yte = d["X_test"], d["y_test"]
    best = best_models[k_classes]
    yhat_te = best.predict(Xte)
    rows.append({
        "dataset_classes": k_classes, "model": "MLP-Optimized", "split": "TEST",
        "accuracy": accuracy_score(yte, yhat_te),
        "f1_macro": f1_score(yte, yhat_te, average="macro"),
        "f1_weighted": f1_score(yte, yhat_te, average="weighted"),
        "best_params": best.get_params()["mlp"]
    })

results = pd.DataFrame(rows).sort_values(["dataset_classes","split","f1_macro"], ascending=[True, True, False])
print(results)

# Vista rápida
print("\nTop DEV por dataset (MLP):")
print(results[results["split"]=="DEV"].loc[
    results[results["split"]=="DEV"].groupby("dataset_classes")["f1_macro"].idxmax(),
    ["dataset_classes","model","f1_macro","accuracy","best_params"]
])
print("\nResultados TEST (MLP optimizado):")
print(results[results["split"]=="TEST"].sort_values(["dataset_classes","f1_macro"], ascending=[True, False]))





# 3. Evaluación
rows = []
conf_matrices = {}

for k_classes, d in datasets.items():
    Xtr, ytr = d["X_train"], d["y_train"]
    Xte, yte = d["X_test"],  d["y_test"]

    for nombre, modelo in modelos.items():
        modelo.fit(Xtr, ytr)
        ypred = modelo.predict(Xte)

        acc = accuracy_score(yte, ypred)
        prec = precision_score(yte, ypred, average="macro")
        rec = recall_score(yte, ypred, average="macro")
        f1m = f1_score(yte, ypred, average="macro")
        f1w = f1_score(yte, ypred, average="weighted")

        rows.append({
            "dataset_classes": k_classes,
            "model": nombre,
            "accuracy": acc,
            "precision_macro": prec,
            "recall_macro": rec,
            "f1_macro": f1m,
            "f1_weighted": f1w
        })

        # Guardamos la matriz de confusión
        conf_matrices[(k_classes, nombre)] = confusion_matrix(yte, ypred)

# 4. Resultados en tabla
results = pd.DataFrame(rows).sort_values(["dataset_classes","f1_macro"], ascending=[True, False])
print(results)

# 5. Ejemplo: mostrar matrices de confusión de cada modelo en 3 clases
print("\nMATRICES DE CONFUSIÓN (dataset 3 clases):")
for nombre in modelos.keys():
    print(f"\nModelo: {nombre}")
    print(conf_matrices[(20, nombre)])


def plot_confusion_heatmap(cm, class_names, title="Matriz de confusión", normalize=False):
    if normalize:
        cm = cm.astype(float) / cm.sum(axis=1, keepdims=True).clip(min=1)
    fig, ax = plt.subplots(figsize=(4,3))
    im = ax.imshow(cm, aspect="auto")
    ax.set_title(title)
    ax.set_xlabel("Predicción")
    ax.set_ylabel("Real")
    ax.set_xticks(range(len(class_names))); ax.set_xticklabels(class_names, rotation=45, ha="right")
    ax.set_yticks(range(len(class_names))); ax.set_yticklabels(class_names)
    # anotaciones
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, f"{cm[i, j]:.2f}" if normalize else f"{cm[i, j]}",
                    ha="center", va="center", fontsize=9, color="white" if im.norm(cm[i,j])>0.5 else "black")
    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    plt.tight_layout()
    plt.show()

# Ejemplo: mostrar heatmaps para TODOS los modelos del dataset de 3 clases
k = 5
class_names = sorted(np.unique(datasets[k]["y_test"]))
for model_name in modelos.keys():
    cm = conf_matrices[(k, model_name)]
    plot_confusion_heatmap(cm, class_names, title=f"{model_name} | {k} clases (sin normalizar)", normalize=False)
    plot_confusion_heatmap(cm, class_names, title=f"{model_name} | {k} clases (normalizada por fila)", normalize=True)


def barplot_metrics(results, k_classes, metrics=("accuracy","precision_macro","recall_macro","f1_macro")):
    df = results[results["dataset_classes"]==k_classes].copy()
    # si tienes filas DEV y TEST, filtra una (ej. TEST) para comparar final
    if "split" in df.columns:
        df = df[df["split"]=="TEST"].copy()
    models = df["model"].tolist()
    x = np.arange(len(models))
    width = 0.18

    fig, ax = plt.subplots(figsize=(12,4))
    for i, m in enumerate(metrics):
        ax.bar(x + i*width, df[m].values, width, label=m)
    ax.set_xticks(x + width*(len(metrics)-1)/2)
    ax.set_xticklabels(models, rotation=30, ha="right")
    ax.set_ylim(0, 1.0)
    ax.set_title(f"Métricas en TEST | {k_classes} clases")
    ax.legend()
    ax.grid(axis="y", alpha=0.3)
    plt.tight_layout()
    plt.show()

# Barras para 2, 3 y 5 clases (puedes cambiar el set de métricas si quieres)
for k in [2,5,20]:
    barplot_metrics(results, k, metrics=("accuracy","precision_macro","recall_macro","f1_macro"))


def plot_decision_boundary(ax, clf, X, y, title=None, proba=False, padding=0.05, grid=300):
    # Rango con pequeño padding
    x_min, x_max = X[:,0].min(), X[:,0].max()
    y_min, y_max = X[:,1].min(), X[:,1].max()
    dx, dy = x_max - x_min, y_max - y_min
    x_min, x_max = x_min - dx*padding, x_max + dx*padding
    y_min, y_max = y_min - dy*padding, y_max + dy*padding

    xx, yy = np.meshgrid(np.linspace(x_min, x_max, grid),
                         np.linspace(y_min, y_max, grid))
    grid_xy = np.c_[xx.ravel(), yy.ravel()]

    # Predicción en rejilla
    if proba and hasattr(clf, "predict_proba"):
        Z = clf.predict_proba(grid_xy).argmax(1)  # clase más probable
    else:
        Z = clf.predict(grid_xy)
    Z = Z.reshape(xx.shape)

    # Mapa de decisión + puntos
    cs = ax.contourf(xx, yy, Z, alpha=0.25, levels=np.unique(Z).size)
    sc = ax.scatter(X[:,0], X[:,1], c=y, s=20, alpha=0.8)
    ax.set_title(title or type(clf).__name__)
    ax.set_xlabel("Feature 1"); ax.set_ylabel("Feature 2")

def visualize_many_classifiers(classifiers, X, y, cols=3, fit=True):
    """
    classifiers: lista de (nombre, estimator) — usa Pipeline con StandardScaler si el modelo lo necesita.
    X, y: datos 2D (si tienes >2 features, proyecta antes con PCA/UMAP).
    """
    n = len(classifiers)
    rows = int(np.ceil(n/cols))
    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 3.5*rows))
    axes = np.array(axes).reshape(-1)

    for i, (name, clf) in enumerate(classifiers):
        if fit:
            clf.fit(X, y)
        plot_decision_boundary(axes[i], clf, X, y, title=name)
    # ocultar subplots vacíos
    for j in range(i+1, rows*cols):
        axes[j].axis("off")
    plt.tight_layout(); plt.show()


# Datos 2D
X_a, y_a = make_blobs(n_samples=1249, centers=2, cluster_std=1.2, random_state=42)
X_b, y_b = make_blobs(n_samples=1249, centers=5, cluster_std=1.2, random_state=42)
X_c, y_c = make_blobs(n_samples=1249, centers=20, cluster_std=1.2, random_state=42)

clfs = [
    
    ("KNN-7",  Pipeline([("sc", StandardScaler()),
                         ("clf", KNeighborsClassifier(n_neighbors=7))])),
    
    ("SVM-RBF",Pipeline([("sc", StandardScaler()),
                         ("clf", SVC(kernel="rbf", C=10, gamma="scale"))])),
    
    ("Árbol",  DecisionTreeClassifier(max_depth=5, random_state=42)),
    
    ("MLP",    Pipeline([("sc", StandardScaler()),
                         ("clf", MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42))])),
]

visualize_many_classifiers(clfs, X_a, y_a, cols=3, fit=True)
visualize_many_classifiers(clfs, X_b, y_b, cols=3, fit=True)
visualize_many_classifiers(clfs, X_c, y_c, cols=3, fit=True)














warnings.filterwarnings("ignore")
RND = 42

RUN_SIZES = [10**2, 10**3, 10**4, 10**5]
USE_1M = False
if USE_1M and 10**6 not in RUN_SIZES:
    RUN_SIZES.append(10**6)

# Maxímo de filas para usar modelos pesados (SVM-RBF, KNN, MLP)
MAX_ROWS_FOR_HEAVY_MODELS = 200_000 # sustitución por modelos lineale






def gen_and_split(n_samples, n_features=2, n_classes=4, random_state=RND):
    X, y = make_gaussian_quantiles(n_samples=n_samples, n_features=n_features,
                                   n_classes=n_classes, random_state=random_state)
    # split 60/20/20 train/dev/test
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=random_state)
    X_dev, X_test, y_dev, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=random_state)
    return X_train, X_dev, X_test, y_train, y_dev, y_test






base_models = {
    "kNN": (KNeighborsClassifier(), {"n_neighbors":[3,5,7]}),
    "DecisionTree": (DecisionTreeClassifier(random_state=RND), {"max_depth":[3,5,10,None], "min_samples_leaf":[1,2,5]}),
    "SVM": (SVC(), {"C":[0.1,1], "kernel":["linear","rbf"]}),
    "MLP": (MLPClassifier(max_iter=300, random_state=RND), {"hidden_layer_sizes":[(50,),(100,)], "learning_rate_init":[0.001]})
}

large_models = {
    "kNN": (KNeighborsClassifier(algorithm='ball_tree', n_jobs=-1), {"n_neighbors":[3,5]}),
    "DecisionTree": (DecisionTreeClassifier(random_state=RND), {"max_depth":[10, None], "min_samples_leaf":[1,5]}),
    "SGD_log": (SGDClassifier(loss='log', max_iter=1000, tol=1e-3, random_state=RND), {"alpha":[1e-4,1e-3]}),  # proxy para SVM/LogReg
}





def run_experiment(sizes, use_large_subs=True, random_state=RND):
    results = []
    details = []  # para guardar reports si quieres
    for n in sizes:
        print(f"\n--- Size: {n} samples ---")
        X_train, X_dev, X_test, y_train, y_dev, y_test = gen_and_split(n)
        n_train = X_train.shape[0]
        print(f"Train/dev/test sizes: {X_train.shape[0]}/{X_dev.shape[0]}/{X_test.shape[0]}")
        
        # Decide los modelos
        if use_large_subs and n > MAX_ROWS_FOR_HEAVY_MODELS:
            models = large_models
            print("Usando configuración para datasets grandes (modelos ligeros).")
        else:
            models = base_models
        
        for name, (model, grid) in models.items():
            print(f" -> Model: {name}")
            # GridSearch sobre X_train
            grid_search = GridSearchCV(model, grid, cv=3, scoring='accuracy', n_jobs=-1, refit=True)
            try:
                grid_search.fit(X_train, y_train)
                best = grid_search.best_estimator_
            except Exception as e:
                print(f"   ERROR al entrenar {name}: {e}. Intentando fit directo con parámetros por defecto.")
                try:
                    model.fit(X_train, y_train)
                    best = model
                except Exception as e2:
                    print(f"   Falló también fit directo: {e2}. Se salta este modelo.")
                    continue

            # Evaluación
            y_dev_pred = best.predict(X_dev)
            y_test_pred = best.predict(X_test)
            acc_dev = accuracy_score(y_dev, y_dev_pred)
            acc_test = accuracy_score(y_test, y_test_pred)
            f1_dev = f1_score(y_dev, y_dev_pred, average='macro')
            f1_test = f1_score(y_test, y_test_pred, average='macro')

            results.append({
                "n_samples": n,
                "model": name,
                "best_params": getattr(grid_search, "best_params_", None),
                "acc_dev": acc_dev,
                "acc_test": acc_test,
                "f1_dev": f1_dev,
                "f1_test": f1_test,
                "n_train": n_train
            })
            # Guardar reportes detallados
            details.append({
                "n_samples": n,
                "model": name,
                "classification_report_test": classification_report(y_test, y_test_pred, zero_division=0),
                "confusion_matrix_test": confusion_matrix(y_test, y_test_pred)
            })
            print(f"    acc_dev={acc_dev:.4f}  acc_test={acc_test:.4f}  f1_test={f1_test:.4f}")

    df = pd.DataFrame(results)
    return df, details






df_results, details = run_experiment(RUN_SIZES, use_large_subs=True)
print("\n== Resultados resumidos ==")
display(df_results)

# Guardado de resultados
df_results, details = run_experiment(RUN_SIZES, use_large_subs=True)





# =========================
# Plot: Barplot de Accuracy (test) por tamaño y modelo
# =========================
plt.figure(figsize=(10,6))
sns.barplot(data=df_results, x="n_samples", y="acc_test", hue="model")
plt.title("Accuracy (test) por modelo y tamaño de dataset")
plt.ylabel("Accuracy (test)")
plt.xlabel("n_samples")
plt.ylim(0,1)
plt.legend(bbox_to_anchor=(1.02,1), loc="upper left")
plt.show()

# =========================
# Heatmap: pivot tabla
# =========================
pivot = df_results.pivot_table(index="model", columns="n_samples", values="acc_test")
plt.figure(figsize=(8,4))
sns.heatmap(pivot, annot=True, fmt=".3f", cmap="viridis")
plt.title("Heatmap: Accuracy (test)")
plt.show()

# =========================
# Radar plot por modelo - comparando tamaños
# =========================
def radar_plot_for_model(df, model_name):
    from math import pi
    d = df[df['model']==model_name].sort_values('n_samples')
    values = d['acc_test'].values
    labels = d['n_samples'].astype(str).tolist()
    N = len(values)
    angles = [n / float(N) * 2 * pi for n in range(N)]
    values = np.concatenate((values, [values[0]]))
    angles = angles + [angles[0]]
    plt.figure(figsize=(6,6))
    ax = plt.subplot(111, polar=True)
    ax.plot(angles, values, linewidth=2)
    ax.fill(angles, values, alpha=0.25)
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(labels)
    ax.set_title(f"Radar: {model_name} (accuracy test)")
    ax.set_ylim(0,1)
    plt.show()


radar_plot_for_model(df_results, "DecisionTree")

radar_plot_for_model(df_results, "kNN")

radar_plot_for_model(df_results, "MLP")

radar_plot_for_model(df_results, "SVM")





















